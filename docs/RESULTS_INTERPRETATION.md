# Results Interpretation Guide

This guide walks you through all fifteen figures generated by the research suite, explaining what each one shows, how to interpret the data, what the key insights are, and how the results apply to practical THz system design. Think of this as your companion document while examining the figures, helping you extract maximum value from the simulation results.

## Table of Contents

- [Basic Analysis Figures (1-9)](#basic-analysis-figures)
  - [Figures 1-2: Antenna Gain Scaling](#figures-1-2-antenna-gain-scaling)
  - [Figures 3-4: Path Loss Characteristics](#figures-3-4-path-loss-characteristics)
  - [Figures 5-6: Shannon Capacity](#figures-5-6-shannon-capacity)
  - [Figure 7: Beam Alignment Strategies](#figure-7-beam-alignment-strategies)
  - [Figure 8: Near-Field Effects](#figure-8-near-field-effects)
  - [Figure 9: 2D versus 3D Alignment](#figure-9-2d-versus-3d-alignment)
- [Monte Carlo Analysis Figures (10-15)](#monte-carlo-analysis-figures)
  - [Figure 10: Capacity Distributions](#figure-10-capacity-distributions)
  - [Figure 11: Environmental Sensitivity](#figure-11-environmental-sensitivity)
  - [Figure 12: Capacity with Uncertainty](#figure-12-capacity-with-uncertainty)
  - [Figure 13: Advanced Beam Strategies](#figure-13-advanced-beam-strategies)
  - [Figure 14: Realistic Impairments](#figure-14-realistic-impairments)
  - [Figure 15: Rain Attenuation](#figure-15-rain-attenuation)
- [Data Files](#data-files)
- [Extracting Design Insights](#extracting-design-insights)

---

## Basic Analysis Figures

These nine figures establish the fundamental behavior of THz wireless systems. They show deterministic relationships without uncertainty, helping you understand the physics before adding the complexity of statistical variation.

### Figures 1-2: Antenna Gain Scaling

**Figure 1: Antenna Array Gain versus Frequency**

This figure shows how antenna gain changes with frequency for several fixed array sizes. You will see that each curve slopes upward with increasing frequency, meaning higher frequencies provide higher gains for the same physical array size.

**What You Are Looking At:** Five curves corresponding to array sizes from one centimeter up to twenty centimeters. The vertical axis shows gain in decibels relative to an isotropic radiator (dBi). The horizontal axis spans one hundred to five hundred gigahertz.

**Key Observations:**
- All curves increase approximately linearly on this scale, rising about six decibels per octave (doubling) of frequency. This happens because higher frequencies have shorter wavelengths, allowing more antenna elements to fit in the same physical area. Since gain scales with number of elements, and number of elements scales with frequency squared (because area contains wavelength-squared more elements), gain increases as frequency squared, appearing as a steady rise on the log scale.
  
- Larger arrays provide consistently higher gain, with the spacing between curves representing the gain advantage. A twenty-centimeter array has sixteen times the area of a five-centimeter array, potentially providing sixteen times more elements and twelve decibels more gain (ten times log of sixteen). The actual difference may be slightly less due to edge effects and mutual coupling.

- At three hundred gigahertz, a ten-centimeter array achieves roughly forty-five decibels of gain. This is sufficient for multi-gigabit links over hundreds of meters, showing why THz frequencies enable compact, high-performance communication.

**Design Insight:** When selecting operating frequency, remember that higher frequency gives more gain per unit physical size, but it also increases path loss and absorption. The net effect on link budget depends on the specific distance and environmental conditions. This figure tells you the gain side of that trade-off.

**Figure 2: Antenna Array Gain versus Array Size**

This complementary figure shows how gain scales with physical array size for several fixed frequencies. Now the horizontal axis is array size in centimeters, and we see multiple curves for different frequencies from one hundred to five hundred gigahertz.

**What You Are Looking At:** Each curve represents one frequency, showing how making the array larger increases gain. The curves fan out because higher frequencies benefit more from increased size.

**Key Observations:**
- At low frequencies like one hundred gigahertz, gain increases relatively slowly with size because even a large array contains only a modest number of wavelengths. A twenty-centimeter array at one hundred gigahertz spans about sixty-seven wavelengths, containing roughly four thousand five hundred elements with lambda-over-two spacing.

- At high frequencies like five hundred gigahertz, the same twenty-centimeter array spans over three hundred wavelengths, containing more than ninety thousand elements and achieving extremely high gain exceeding sixty decibels. This is why THz systems can use compact arrays yet still close links over significant distances.

- All curves show the characteristic relationship where doubling array size (moving rightward by a factor of two on the horizontal axis) increases gain by six decibels. This comes from the area quadrupling when linear dimension doubles, and four times more elements means four times more gain or six decibels.

**Design Insight:** There is a diminishing return to making arrays larger. Going from five to ten centimeters gives six decibels of gain, but going from ten to twenty centimeters (twice as much additional size) only gives another six decibels. Meanwhile, costs, mechanical challenges, and near-field distances all increase. Most practical systems settle on array sizes in the five-to-fifteen-centimeter range as a reasonable compromise.

---

### Figures 3-4: Path Loss Characteristics

**Figure 3: Total Path Loss versus Frequency**

This figure reveals why frequency selection is so critical in THz systems. It shows total path loss (free-space spreading plus molecular absorption) as a function of frequency for several fixed distances.

**What You Are Looking At:** Multiple curves showing path loss from one hundred to five hundred gigahertz. Each curve corresponds to a different link distance from ten meters to one kilometer. The vertical axis shows loss in decibels; higher values mean worse propagation.

**Key Observations:**
- The curves are not smooth. They show peaks and valleys corresponding to molecular absorption resonances. The prominent peaks around 118 gigahertz (oxygen), 183 gigahertz (water vapor), and 325 gigahertz (oxygen again) are clearly visible as sharp increases in path loss.

- Between these absorption peaks lie "quiet windows" where loss is relatively low. The region from 200 to 240 gigahertz is particularly attractive, showing as a dip or plateau in the curves. This is why many THz system designs target this frequency range.

- Longer distances show more dramatic absorption peaks because absorption loss scales linearly with distance. A ten-meter link might hardly notice the absorption peaks, but a one-kilometer link sees twenty to forty decibels more loss at peak frequencies compared to quiet windows. This frequency selectivity means that for long-range links, you must carefully choose your operating frequency to avoid absorption peaks.

- The baseline rise in path loss with frequency (ignoring the absorption peaks) comes from the free-space path loss term, which increases twenty decibels per decade of frequency. Even in a perfect vacuum, higher frequencies suffer more path loss because the same antenna physical size captures less of the spreading wavefront when wavelength is smaller.

**Design Insight:** For distances beyond one hundred meters, molecular absorption dominates the link budget. You cannot simply pick any frequency in the THz range; you must target a quiet window or accept severe absorption penalties. For shorter links under fifty meters, absorption matters less, and you can prioritize other factors like spectrum availability or hardware maturity.

**Figure 4: Total Path Loss versus Distance**

This complementary view shows how path loss increases with distance for several fixed frequencies. Now the horizontal axis is distance on a logarithmic scale from one meter to one kilometer.

**What You Are Looking At:** Five or six curves, each representing one frequency. All curves increase as we move rightward (increasing distance), but some increase faster than others.

**Key Observations:**
- All curves start bunched together at short distances and diverge as distance increases. This happens because at short distances, free-space path loss dominates and is similar for all frequencies. At longer distances, molecular absorption accumulates and becomes the differentiator.

- Frequencies in quiet windows (like 140 gigahertz or 200 gigahertz) show relatively gentle curves that closely follow the forty-decibels-per-decade slope expected from free-space path loss doubling the distance increases loss by six decibels. These frequencies are usable out to hundreds of meters.

- Frequencies near absorption peaks (like 183 gigahertz or 380 gigahertz) show much steeper curves. The 183-gigahertz line might increase by fifty or sixty decibels per decade rather than forty, making the usable range much shorter. By five hundred meters, these frequencies may be completely impractical.

- The crossover point where absorption begins to dominate over free-space path loss varies with frequency but typically occurs around fifty to one hundred meters for mid-THz frequencies. Design systems for shorter distances can often ignore absorption; systems for longer distances absolutely cannot.

**Design Insight:** The logarithmic distance scale lets you quickly assess feasibility. If your target distance is one hundred meters and a frequency shows one hundred twenty decibels of path loss, but your link budget (transmit power plus antenna gains) is only one hundred decibels, that frequency will not close the link. Look for a frequency where path loss stays below your link budget out to your required distance.

---

### Figures 5-6: Shannon Capacity

These figures translate the physical layer characteristics (antenna gains and path loss) into information-theoretic performance: how many bits per second can you reliably transmit?

**Figure 5: Shannon Capacity versus Frequency**

This figure shows achievable data rates as a function of frequency for several distances, assuming fixed array sizes (typically five centimeters), transmit power (ten dBm), and bandwidth (ten gigahertz).

**What You Are Looking At:** Three curves for distances of ten, fifty, and one hundred meters. The vertical axis shows capacity in gigabits per second on a logarithmic scale. The horizontal axis spans one hundred to three hundred gigahertz.

**Key Observations:**
- At short distances like ten meters, capacity is generally high (hundreds of gigabits per second) and relatively flat across frequency. This is because at short range, signal-to-noise ratio is so high that we are in the logarithmic region of Shannon's formula where capacity grows slowly with SNR. The system is limited more by bandwidth than by SNR.

- At longer distances like one hundred meters, capacity varies dramatically with frequency due to molecular absorption. Peaks in capacity correspond to quiet windows (around 140 gigahertz and 200-220 gigahertz), while valleys correspond to absorption peaks. At 183 gigahertz, capacity might drop by an order of magnitude compared to 200 gigahertz because absorption reduces SNR so severely.

- All three curves show the general trend of increasing capacity with frequency (thanks to antenna gain), modulated by absorption effects. The net result is a "sawtooth" pattern where capacity rises in quiet windows and crashes at absorption peaks.

- Notice that even at one hundred meters in a good frequency window, you can achieve fifty to one hundred gigabits per second with modest five-centimeter arrays. This demonstrates why THz is attractive for short-range high-capacity applications like wireless backhaul and device-to-device links.

**Design Insight:** For capacity-critical applications, frequency selection based on this curve is essential. A poor frequency choice can cost you fifty percent or more of your potential capacity. The "sweet spot" frequencies offering best capacity for typical distances are 140-150 gigahertz and 200-240 gigahertz.

**Figure 6: Shannon Capacity versus Distance**

This shows how capacity degrades as you move transmitter and receiver farther apart, for several array sizes at a fixed frequency (typically 200 gigahertz).

**What You Are Looking At:** Three curves representing two-centimeter, five-centimeter, and ten-centimeter arrays. Both axes are logarithmic, creating a characteristic straight-line appearance for each curve. Capacity decreases (moving downward) as distance increases (moving rightward).

**Key Observations:**
- The curves are roughly parallel, meaning the relative advantage of larger arrays remains constant with distance. A ten-centimeter array provides about ten to fifteen decibels more link budget than a five-centimeter array, translating to roughly double the range for the same capacity target, or roughly four times the capacity at the same range.

- The slope of these curves tells you how sensitive capacity is to distance. In the middle distance range (twenty to two hundred meters), capacity typically drops by a factor of two to three for each doubling of distance. This is steeper than the inverse-square relationship of power because capacity depends logarithmically on SNR, which itself depends linearly on received power.

- At very short distances (under ten meters), curves flatten because SNR is so high that we hit the logarithmic region of Shannon's formula. Adding more distance reduces SNR, but capacity barely changes because log₂(1+SNR) is insensitive to SNR when SNR is large. You need a huge SNR reduction to see significant capacity loss.

- At very long distances (beyond a few hundred meters for these array sizes), curves become steeper as absorption begins to dominate. Eventually, capacity approaches zero as SNR drops below one.

**Design Insight:** This figure helps you specify array size requirements. If you need fifty gigabits per second at one hundred meters, trace horizontally from fifty gigabits per second until you hit the one-hundred-meter vertical line, then see which curve you intersect. That tells you the minimum array size needed.

---

### Figure 7: Beam Alignment Strategies

This figure compares four different strategies for initial beam alignment in two-dimensional space (azimuth only). It reveals a key patent-relevant innovation: hierarchical search dramatically outperforms traditional approaches for narrow beams.

**What You Are Looking At:** The left panel shows mean alignment time versus beamwidth for four strategies: clockwise sweep, random search, binary search, and adaptive step. The right panel shows probability density functions (histograms) of alignment times for a specific beamwidth (typically twenty degrees).

**Key Observations in Left Panel:**
- Clockwise and counterclockwise sweeps (only clockwise is shown because they are symmetric) produce identical curves that decrease linearly as beamwidth increases. This makes intuitive sense: wider beams cover more angular space per step, so fewer steps are needed. At five-degree beamwidth, mean time is around one hundred milliseconds; at forty-degree beamwidth, it drops to about fifteen milliseconds.

- Random search shows a similar trend but with higher average time, especially for narrow beams. The high variance of random search (visible in the right panel) makes it unattractive despite similar mean performance.

- Binary search produces a nearly flat line that is independent of beamwidth. Whether beamwidth is five degrees or forty degrees, binary search takes roughly ten to fifteen milliseconds. This logarithmic behavior means binary search is dramatically superior for narrow beams. At five-degree beamwidth, binary search is ten times faster than systematic sweep. At one-degree beamwidth (not shown but following the same trends), it would be fifty to one hundred times faster.

- Adaptive step (coarse-then-fine search) falls between binary and systematic, offering some improvement without the complexity of full binary implementation.

**Key Observations in Right Panel:**
- The probability distributions show why variance matters. Systematic clockwise sweep has a triangular distribution ranging from near zero (if we start pointing almost at the receiver) to two hundred milliseconds or more (if we start pointing opposite). The mean is around the middle, but worst case is twice the mean.

- Random search has an exponential-looking tail, with most trials completing quickly but some unfortunate trials taking extreme times. This high variance makes latency unpredictable.

- Binary search (if shown) would have a tight distribution concentrated around ten to fifteen milliseconds, with very little variance. This predictability is valuable for real-time systems with latency requirements.

**Design Insight:** For systems with beamwidths below ten degrees (typical for arrays larger than three centimeters at THz frequencies), binary or hierarchical search is essential. The ten-to-hundred-times speedup directly translates to faster network joining, lower latency, and better user experience. This is one of the key innovations suitable for patent protection.

---

### Figure 8: Near-Field Effects

This figure addresses one of the most surprising aspects of THz systems: the near-field region extends much farther than most people expect, affecting beam alignment, link budget calculations, and system design.

**What You Are Looking At:** Two panels. The left panel shows maximum phase deviation versus distance for several array sizes. The right panel shows Fraunhofer distance versus frequency for a fixed array size.

**Key Observations in Left Panel:**
- The curves decrease as distance increases, showing that phase deviation (vertical axis, in degrees) becomes smaller when transmitter and receiver are farther apart. This happens because the wavefront flattens out with distance.

- A horizontal red dashed line marks twenty-two point five degrees, the π/8 criterion for far-field behavior. Where each curve crosses this line defines the Fraunhofer distance for that array size.

- Larger arrays (sixteen by sixteen elements) have curves that stay above the threshold line longer, pushing their Fraunhofer distance out to over one hundred meters at two hundred gigahertz. Smaller arrays (four by four elements) reach far-field conditions at much shorter distances, perhaps fifteen meters.

- The logarithmic scales on both axes mean the relationship is roughly inverse: doubling distance reduces phase deviation by half. This inverse relationship comes from the path-difference formula being proportional to array-size-squared over distance.

**Key Observations in Right Panel:**
- Fraunhofer distance increases linearly with frequency for a fixed physical array size (eight by eight elements in this example). At one hundred gigahertz, the far-field threshold might be twenty meters; at five hundred gigahertz, it stretches to one hundred meters. This happens because higher frequency means more wavelengths fit in the same physical size, effectively making the array "electrically larger."

- The shaded region below the curve represents the near-field zone. Many practical links fall in this zone, especially for short-range high-capacity applications. This does not mean the link fails; it just means you need different signal processing than the standard far-field beamforming assumes.

- The steep rise with frequency highlights a potential pitfall: as you move to higher THz frequencies to get more bandwidth and higher antenna gain, you also dramatically extend the near-field region. A link that was comfortably in the far-field at one hundred gigahertz might be deep in the near-field at five hundred gigahertz.

**Design Insight:** Always check whether your intended link distance is above or below the Fraunhofer distance for your array and frequency. If below, you need near-field beamforming techniques, which account for wavefront curvature. If well above, standard beamforming works fine. The crossover region (distance within a factor of two of Fraunhofer distance) requires careful analysis.

---

### Figure 9: 2D versus 3D Alignment

This figure extends the beam alignment analysis to three dimensions, showing how much harder the problem becomes when you must search both azimuth and elevation rather than azimuth alone.

**What You Are Looking At:** Two curves on a single panel, both showing mean alignment time versus beamwidth. The blue curve represents two-dimensional search (azimuth only); the red curve represents three-dimensional search (azimuth plus elevation).

**Key Observations:**
- The 3D curve lies consistently above the 2D curve by a factor of approximately two point five to three. This means 3D alignment takes roughly three times as long as 2D alignment for the same beamwidth. The penalty comes from having to search an additional dimension with roughly the same angular extent (three hundred sixty degrees in azimuth versus one hundred eighty degrees in elevation).

- Both curves decrease with increasing beamwidth, but the 3D curve decreases more slowly. This suggests that 3D alignment benefits even more than 2D from hierarchical search strategies that can efficiently partition the search space.

- The logarithmic vertical scale emphasizes how severe the problem becomes for narrow beams. At five-degree beamwidth, 3D alignment might take several hundred milliseconds using naive strategies. For mobile applications or networks with frequent handoffs, this latency is problematic.

- The gap between 2D and 3D curves is smaller at wide beamwidths and larger at narrow beamwidths. This happens because with wide beams, elevation search requires fewer steps, but with narrow beams, you must search nearly the full elevation range.

**Design Insight:** For mobile deployments or any application where antenna orientation is uncertain, 3D alignment is unavoidable. The three-times-longer alignment time must be budgeted in your network joining latency requirements. Alternatively, consider using wide-beam omnidirectional initial acquisition followed by beam refinement, trading off some initial capacity for faster joining.

---

## Monte Carlo Analysis Figures

These six figures add statistical rigor by showing not just average performance but full distributions, uncertainty bounds, and sensitivity to various parameters. They transform deterministic predictions into reliable probability statements suitable for system design and capacity planning.

### Figure 10: Capacity Distributions

This four-panel figure shows the probability distributions of capacity for four different frequencies, all with the same distance and array size. It reveals how much variability to expect from real-world uncertainties.

**What You Are Looking At:** Four histograms, one per frequency (150, 200, 250, 300 gigahertz typically). Each histogram shows how many simulation trials fell into each capacity bin. Vertical lines mark the mean (red dashed) and median (blue dashed), while shaded regions show ninety-percent confidence intervals.

**Key Observations:**
- The distributions are approximately Gaussian (bell-shaped) but with slight right skew. This shape arises because multiple independent random perturbations (transmit power, pointing errors, environmental variations) combine, and the central limit theorem pushes the result toward normality.

- Mean and median are close but not identical, confirming the slight skewness. Median is usually a bit lower than mean, meaning more than half the trials fall below the mean. For capacity planning, median or even fifth percentile may be more relevant than mean.

- The standard deviation (shown in the title of each panel) quantifies spread. Typical values are one to three gigabits per second for mean capacities of twenty to sixty gigabits per second, representing five-to-ten-percent variability. This is significant enough that single-point calculations could be misleading.

- Outage probability (also in the title) tells you what fraction of trials failed to meet a threshold (typically one gigabit per second). For good frequencies at reasonable distances, outage probability should be near zero. Higher values indicate marginal link conditions.

- Comparing across frequencies, you will see that some have higher mean capacity (better frequency choice) while others have wider distributions (more sensitive to uncertainties). The ideal frequency has both high mean and low variance.

**Design Insight:** When comparing frequency options, do not just look at mean capacity. A frequency with slightly lower mean but much lower variance might be preferable because it provides more predictable performance. The ninety-percent confidence interval tells you the range you can reliably expect.

---

### Figure 11: Environmental Sensitivity

This three-panel figure shows how capacity varies with temperature, humidity, and pressure. It identifies which environmental factors matter most and over what range of conditions the system remains viable.

**What You Are Looking At:** Three separate plots with environmental parameter on horizontal axis and mean capacity on vertical axis. Each point represents the mean capacity from two hundred Monte Carlo trials at that environmental condition.

**Key Observations in Temperature Panel:**
- Capacity typically increases slightly with decreasing temperature. This happens for two reasons: thermal noise decreases (giving slightly better SNR), and molecular absorption tends to be somewhat lower at colder temperatures. The effect is modest, perhaps a few percent change across the temperature range from minus twenty to plus forty degrees Celsius.

- The relationship is nearly linear, making it easy to predict performance at different temperatures. If you know capacity at twenty degrees Celsius, you can estimate capacity at minus ten degrees Celsius by simple extrapolation.

- The small magnitude of the temperature effect (compared to humidity) tells you that temperature compensation in the link budget is usually not critical unless operating at extreme temperatures.

**Key Observations in Humidity Panel:**
- This typically shows the strongest sensitivity of the three environmental parameters. Capacity can drop by twenty to fifty percent as humidity increases from ten percent to ninety percent, especially at frequencies with significant water vapor absorption (above 200 gigahertz).

- The curve often shows steeper slope at high humidity, indicating nonlinear effects. This happens because water vapor absorption does not scale perfectly linearly with humidity due to pressure broadening and other molecular effects.

- Dry climates (humidity below thirty percent) show substantially better performance than humid climates (humidity above seventy percent). This geographic dependence means system design must account for local climate.

**Key Observations in Pressure Panel:**
- Pressure typically shows the weakest sensitivity of the three parameters. Capacity might vary by only five to ten percent across the pressure range from eighty-five to one hundred five kilopascals (corresponding to sea level versus two thousand meters altitude).

- The curve shape depends on whether oxygen or water vapor absorption dominates at the chosen frequency. At frequencies near oxygen peaks (118, 325 gigahertz), higher pressure means more absorption and lower capacity. At frequencies dominated by water vapor absorption, pressure has minimal effect.

**Design Insight:** For outdoor deployments, accounting for humidity variations is critical. A link budget calculated assuming fifty percent humidity will be overly optimistic in tropical climates with eighty to ninety percent humidity and conservative in desert climates with twenty to thirty percent humidity. Temperature and pressure compensation are less critical but may be included for the most demanding applications.

---

### Figure 12: Capacity with Uncertainty

This figure combines the deterministic capacity-versus-distance relationship from Figure 6 with uncertainty bounds from Monte Carlo analysis. It shows not just the mean capacity at each distance, but also the range of variation you should expect.

**What You Are Looking At:** Three curves (for three array sizes) showing mean capacity versus distance, with shaded bands around each curve representing the fifth-to-ninety-fifth-percentile range. Both axes are logarithmic.

**Key Observations:**
- The shaded bands are narrowest at short distances (where SNR is high and uncertainties have small relative impact) and widest at long distances (where SNR is low and uncertainties cause larger percentage variations in capacity). This happens because Shannon capacity involves a logarithm, which compresses variations at high SNR but amplifies them at low SNR.

- The ninety-percent confidence bands (from fifth to ninety-fifth percentile) typically span a factor of one point five to two in capacity. For example, if mean capacity is thirty gigabits per second, the fifth percentile might be twenty gigabits per second and ninety-fifth percentile forty-five gigabits per second. This spread is significant enough that you should not design to mean capacity; instead, design to fifth or tenth percentile to ensure adequate performance ninety or ninety-five percent of the time.

- Comparing across array sizes, you see that larger arrays not only provide higher mean capacity (curves higher on the plot) but also slightly tighter relative uncertainty (narrower bands as a fraction of mean). This happens because larger arrays have higher gain, moving the link farther into the high-SNR regime where logarithmic effects compress variations.

- The crossover distance where capacity drops below some threshold varies substantially between fifth percentile and ninety-fifth percentile. For example, mean capacity might cross below ten gigabits per second at three hundred meters, but fifth percentile crosses at two hundred meters while ninety-fifth percentile holds on until four hundred meters. This spread affects range planning.

**Design Insight:** Always design for a low percentile (fifth or tenth) rather than mean. If you need to guarantee thirty gigabits per second for ninety-five percent of conditions, find where the fifth-percentile curve (lower boundary of the shaded band) crosses thirty gigabits per second. That is your reliable range. Designing to mean capacity will cause outages five percent or more of the time.

---

### Figure 13: Advanced Beam Strategies

This figure focuses on comparing the innovative beam alignment strategies (binary search and adaptive step) introduced in this research. It shows that sophisticated strategies dramatically outperform naive approaches, especially for narrow beams typical of large THz arrays.

**What You Are Looking At:** Two panels similar to Figure 7. The left panel shows mean alignment time versus beamwidth for advanced strategies. The right panel shows probability distributions for a specific beamwidth.

**Key Observations in Left Panel:**
- Binary search maintains nearly constant alignment time (ten to twenty milliseconds) across the entire beamwidth range. This is the key innovation: performance independent of beamwidth. Traditional strategies get linearly worse as beams narrow; binary search remains constant.

- Adaptive step performs between binary and traditional, offering perhaps a three-to-five-times speedup compared to systematic sweep at narrow beamwidths. This middle ground may be attractive for implementations where binary search is too complex but some improvement over naive sweep is desired.

- At wide beamwidths (above thirty degrees), all strategies converge because the problem becomes easy regardless of approach. The differences emerge at narrow beamwidths (below fifteen degrees) where THz systems typically operate.

**Key Observations in Right Panel:**
- The probability distributions for binary and adaptive strategies are much tighter than for systematic sweep. Most trials complete near the mean, with very few extending to long times. This low variance makes latency predictable, important for real-time applications.

- Binary search shows almost no trials exceeding thirty milliseconds, while systematic sweep has a long tail extending to one hundred milliseconds or more. For a system with a fifty-millisecond joining latency requirement, binary search succeeds essentially one hundred percent of the time while systematic sweep fails thirty to forty percent of the time.

**Design Insight:** The choice of alignment strategy has first-order impact on system usability. A factor-of-ten speedup in joining time transforms user experience, especially for mobile scenarios with frequent handoffs. Implementing binary or adaptive search should be a high priority for any THz system with beamwidths below ten degrees.

---

### Figure 14: Realistic Impairments

This figure compares ideal antenna gain (assuming perfect efficiency, no mutual coupling, perfect alignment) with realistic gain including actual impairments. It quantifies how much performance you lose when moving from theory to practice.

**What You Are Looking At:** Two curves showing gain versus frequency. The blue curve represents ideal conditions; the red dashed curve represents realistic conditions. The gap between them (shaded) represents total loss from impairments.

**Key Observations:**
- The realistic curve lies consistently below the ideal curve by three to six decibels across the frequency range. This gap represents the combined effect of eighty percent efficiency (one decibel loss), half-decibel mutual coupling loss, and two-degree pointing error (one to three decibels loss depending on frequency and array size).

- The gap grows with frequency because pointing errors have larger impact at higher frequencies where beamwidths are narrower. A two-degree pointing error might cause only one decibel loss at one hundred gigahertz but three decibels loss at five hundred gigahertz.

- Even with impairments, realistic gain still increases with frequency (upward slope of red curve), just not as fast as ideal gain. This means higher frequencies remain attractive despite sensitivity to impairments.

- The magnitude of impairment loss (three to six decibels) is significant compared to typical link margins (ten to twenty decibels). Ignoring impairments in link budget calculations could lead to designs that fail in practice.

**Design Insight:** Always include realistic impairments in your link budgets. The difference between ideal and realistic is not a small correction; it is three to six decibels, equivalent to halving or quartering the available power. Systems designed assuming ideal gains will be disappointed by field performance.

---

### Figure 15: Rain Attenuation

This final figure adds another layer of realism by showing how rain affects THz links. Rain is one of the most severe impairments for outdoor THz systems, potentially causing tens of decibels of additional loss.

**What You Are Looking At:** Four curves showing capacity versus distance for rain rates of zero, five, ten, and twenty-five millimeters per hour. All curves decrease as distance increases, but rainy curves decrease faster.

**Key Observations:**
- The no-rain curve serves as a baseline, showing the capacity you would expect in clear conditions. This matches the deterministic capacity-versus-distance curves from earlier figures.

- Light rain (five millimeters per hour, drizzle) causes moderate additional loss, perhaps three to five decibels, reducing capacity by twenty to thirty percent at medium ranges. The curve shifts downward but retains roughly the same shape.

- Moderate rain (ten millimeters per hour, light rain) causes more severe loss, perhaps eight to twelve decibels, halving or quartering capacity at ranges beyond fifty meters. The curve not only shifts downward but also steepens, indicating that rain impact grows with distance.

- Heavy rain (twenty-five millimeters per hour, moderate downpour) essentially blocks THz communication beyond very short ranges. Capacity drops precipitously with distance, crossing below one gigabit per second at ranges as short as fifty meters. The curve may show a "knee" where capacity collapses.

- The relative impact of rain depends on frequency. At frequencies with already high molecular absorption (near water vapor peaks), adding rain matters less relatively because absorption already dominates. At quiet-window frequencies, rain introduces a new dominant loss mechanism, dramatically changing link behavior.

**Design Insight:** Outdoor THz systems must account for rain in their availability calculations. If you need ninety-nine-point-nine percent availability and it rains (more than five millimeters per hour) one percent of the time, you must either over-provision the link to maintain capacity in rain, or accept degraded service during rain, or use hybrid systems with a lower-frequency (millimeter wave) backup for rainy conditions.

---

## Data Files

In addition to figures, the research suite exports several CSV (comma-separated value) files containing the underlying numerical data. These files allow you to perform further analysis, import results into other tools, or verify the figures against your own calculations.

### Monte Carlo Raw Data Files

**Format:** `thz_data_mc_<label>_<timestamp>.csv`

**Contents:** Each row represents one Monte Carlo trial, with columns for capacity (gigabits per second), SNR (decibels), and received power (dBm). This raw data lets you compute your own statistics, create custom visualizations, or combine results with other data sources.

**When to Use:** If you want to calculate statistics beyond what the figures show (like ninety-ninth percentile, coefficient of variation, or conditional distributions), load this CSV into your analysis tool of choice.

### Monte Carlo Statistics Files

**Format:** `thz_data_mc_stats_<label>_<timestamp>.csv`

**Contents:** Summary statistics including mean, median, standard deviation, minimum, maximum, fifth percentile, ninety-fifth percentile, outage probability, and mean SNR. This gives you a quick numerical summary without needing to process the full raw data.

**When to Use:** For tabulating results, comparing scenarios, or reporting performance metrics in a paper or presentation.

### Link Budget Files

**Format:** `thz_data_linkbudget_<label>_<timestamp>.csv`

**Contents:** Complete link budget breakdown showing transmit power, transmit gain, receive gain, path loss components (free-space, absorption, rain), received power, noise power, SNR, and capacity. This is a full audit trail showing how the capacity calculation was performed.

**When to Use:** For debugging link budgets, verifying calculations against hand analysis, or understanding which loss terms dominate for your specific configuration.

### Beam Alignment Files

**Format:** `thz_data_alignment_<strategy>_bw<beamwidth>_<timestamp>.csv`

**Contents:** One row per simulation trial showing the alignment time in milliseconds. This data underlies the histograms in Figures 7, 9, and 13.

**When to Use:** For computing percentiles beyond fifth and ninety-fifth, analyzing the full probability distribution, or testing the goodness-of-fit to theoretical models.

---

## Extracting Design Insights

The real value of these figures lies not just in understanding what they show, but in applying that understanding to make better design decisions. Here are some key insights organized by design question:

### How do I choose operating frequency?

**Consult:** Figures 3, 5, and 11 (humidity panel)

**Process:** Identify your target distance. On Figure 3, find curves for that distance and look for frequency regions with low path loss (valleys in the curve). These are quiet windows. On Figure 5, confirm that those frequencies also provide adequate capacity for your target data rate. On Figure 11, check humidity sensitivity if you will operate in varying climates. Select a frequency in a quiet window with good capacity and acceptable humidity sensitivity.

**Typical Outcome:** For distances under one hundred meters in temperate climates, 200-240 gigahertz offers best compromise. For longer distances or humid climates, consider 140-150 gigahertz. For very short range (under twenty meters) in controlled environments, higher frequencies above 300 gigahertz are viable.

### What antenna array size do I need?

**Consult:** Figures 2, 6, and 12

**Process:** Identify your target capacity and distance. On Figure 6, trace from your target capacity on the vertical axis to your distance on the horizontal axis. This tells you which array-size curve you need to intersect. If you need high reliability, use Figure 12 instead and aim for the lower edge of the confidence band. Figure 2 then shows you the physical size corresponding to the required gain at your chosen frequency.

**Typical Outcome:** For fifty gigabits per second at one hundred meters and 200 gigahertz, you need roughly eight to twelve centimeter arrays. For one hundred gigabits per second at the same distance, you need fifteen to twenty centimeter arrays. Remember that arrays on both transmitter and receiver contribute to link budget, so doubling one array size gives six decibels of additional link budget.

### How important is beam alignment speed?

**Consult:** Figures 7, 9, and 13

**Process:** Estimate your array size and operating frequency, then calculate beamwidth using the formula from Figure 2 (roughly seventy times wavelength divided by array size). Find that beamwidth on the horizontal axis of Figure 7 and read off the alignment time for your chosen strategy. Multiply by three if you need 3D alignment (Figure 9). Compare this alignment time to your application latency requirements.

**Typical Outcome:** For beamwidths below ten degrees (common for arrays above three centimeters), binary or adaptive search is essential to keep alignment under fifty milliseconds. For video applications or real-time services, even fifty milliseconds may be too long, suggesting you need even faster strategies or wide-beam initial acquisition followed by beam refinement.

### What link margin should I budget?

**Consult:** Figures 10, 12, and 14

**Process:** From Figure 10, note the typical standard deviation (one to three gigabits per second) and outage probability. From Figure 12, measure the gap between mean and fifth-percentile curves at your target distance. From Figure 14, note the typical impairment losses (three to six decibels). Add these together: roughly five to eight decibels for impairments plus another three to six decibels to move from mean to fifth percentile, totaling eight to fifteen decibels. This is your recommended link margin.

**Typical Outcome:** Budget at least ten decibels of link margin beyond your mean calculation. Fifteen decibels is safer for outdoor deployments subject to rain. Systems with less margin will experience frequent outages when conditions deviate from average.

### Can I operate in near-field?

**Consult:** Figure 8

**Process:** Calculate or look up the Fraunhofer distance for your array size and frequency using Figure 8. Compare this to your intended link distance. If your distance is less than the Fraunhofer distance, you are in near-field. If your distance is more than twice the Fraunhofer distance, you are safely in far-field. In between is a transition region requiring careful analysis.

**Typical Outcome:** For ranges under thirty meters with five-to-ten-centimeter arrays at THz frequencies, you are probably in near-field. Standard far-field beamforming will work sub-optimally. Consider near-field focusing techniques or accept a few decibels of loss from using far-field algorithms in near-field conditions.

---

**Last Updated:** January 2026 | **Version:** 1.0

This guide will be updated as new figures and analyses are added to the research suite. If you have questions about interpreting results not covered here, please open an issue on the GitHub repository.
